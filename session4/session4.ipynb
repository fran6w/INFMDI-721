{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "- trouver/supprimer les données dupliquées : df.duplicated() / df.drop_duplicates()\n",
    "- renommer des colonnes : df.rename(columns={...))\n",
    "- trouver les NaN : df.isnull() / df.notnull() / df.dropna()\n",
    "- travail sur les chaines : series.str.extract(), series.str.contains(), series.get_dummies()\n",
    "- mapping : series.map()\n",
    "- changer le type d'une série (cast) : df.astype(type) / pd.to_numeric() / pd.to_datetime()\n",
    "- remplacer n'importe quelle valeur : df.replace({...})\n",
    "- remplacer les NaN : df.fillna(), series.combine_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat people.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement et analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('people.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplications\n",
    "\n",
    "- duplicated() : True ou False selon si une ligne est dupliquée\n",
    "- drop_duplicates() : suppression des lignes dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lignes dupliquées\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toutes les lignes dupliquées\n",
    "df.loc[df.duplicated(keep=False)].sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des lignes dupliquées\n",
    "df = df.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renommage de la colonne 'email address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renommer les colonnes\n",
    "df = df.rename(columns={'email address': 'email'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données manquantes\n",
    "\n",
    "numpy.nan est utilisé dans pandas pour représenter des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not A Number\n",
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "type(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.sqrt(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au passage infinis numpy\n",
    "np.NINF, np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests sur les données manquantes\n",
    "\n",
    "- isnull() ou isna()\n",
    "- notnull() ou notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher les first_name Nan\n",
    "df.loc[df['first_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher tous les lignes avec au moins un NaN\n",
    "df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de toutes les lignes avec un NaN\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de toutes les lignes avec un NaN\n",
    "df.dropna().isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer uniquement les lignes dont le first_name NaN \n",
    "df = df.dropna(subset=['first_name'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout d'une colonne 'full_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_name'  = 'first_name last_name'\n",
    "df['full_name'] = df['first_name'] + ' ' + df['last_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de address\n",
    "df['address'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout des colonnes 'city' et 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de city et country à partir de address\n",
    "df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique : modalités par colonne\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender\n",
    "mapping = {'Female': 'F', 'Male': 'M'}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender, map() avec une Series\n",
    "mapping = pd.Series({'Female': 'F', 'Male': 'M'})\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender avec une Series\n",
    "df['gender'] = df['gender'].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender NaN\n",
    "len(df.loc[df['gender'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse prénom avec gender NaN\n",
    "df.loc[df['gender'].isnull(), 'first_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender\n",
    "df.loc[df['gender'].isnull(), 'first_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter le genre :\n",
    "\n",
    "1. Autocomplétion avec le fichier people.csv (mais très peu de cas)\n",
    "2. Gender API : https://gender-api.com/fr (simple mais API payante si gros volumes + de 500/mois)\n",
    "3. US SSA baby names : https://www.ssa.gov/oact/babynames/limits.html (\"gratuit\", stats à produire, éventuellement affiner par année de naissance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fichier people.csv\n",
    "for first_name in df.loc[df['gender'].isnull(), 'first_name'].unique():\n",
    "    print(first_name, df.loc[df['first_name']==first_name, 'gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de l'âge\n",
    "\n",
    "pandas.Series.astype() : types\n",
    "\n",
    "pandas.to_numeric() : data avec gestion des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de l'âge\n",
    "df.loc[df['age'].str.contains('[^0-9]'), 'age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement de l'âge\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement des dates\n",
    "\n",
    "\n",
    "pandas.to_datetime() : data, gestion des formats et des erreurs\n",
    "\n",
    "pandas.Series.combine_first() : équivalent à fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion des dates\n",
    "df['registration'] = pd.to_datetime(df.registration)\n",
    "df['last_seen'] = pd.to_datetime(df.last_seen, unit='s')\n",
    "# si last_seen est NaN, prendre registration\n",
    "df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "# idem\n",
    "df['last_seen'] = df['last_seen'].combine_first(df['registration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# échantillon\n",
    "np.random.seed(0)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement de 'currency'\n",
    "\n",
    "Produire une nouvelle colonne numérique 'money_eur'.\n",
    "\n",
    "Pour la conversion USD/EUR, on utilise l'API https://api.exchangeratesapi.io/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://api.exchangeratesapi.io/latest')\n",
    "rates = json.loads(response.content)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction de la currency\n",
    "df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "# conversion des monnaies en euros\n",
    "rates['rates']['EUR'] = 1.0\n",
    "df['money_eur'] = df['money_eur'] * df['currency'].map(rates['rates'])\n",
    "np.random.seed(0)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des emails\n",
    "\n",
    "On va utiliser des regex pour nettoyer les emails mais mieux vaut utiliser une librairie spécialisée. Par exemple, https://github.com/syrusakbary/validate_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email NaN\n",
    "df['email'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails absents\n",
    "df = df.dropna(subset=['email']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec chars non admis\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des blancs\n",
    "df['email'] = df['email'].str.strip()\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pour vérifier les domaines\n",
    "df.loc[~df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec noms de domaine invalides\n",
    "df = df.loc[df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec aliases (char +)\n",
    "df.loc[df['email'].str.contains('\\+'), 'email'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des aliases (char +)\n",
    "tab_email = df['email'].str.extract('([^+]+)(\\+.*)?(@.+)').sort_values(0)\n",
    "tab_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprresion des aliases (char +)\n",
    "df['email'] = tab_email[0] + tab_email[2]\n",
    "df.sort_values('email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails en double, on conserve la première ligne\n",
    "df = df.drop_duplicates(subset=['email'])\n",
    "df.sort_values('email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'preference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modalités de preference\n",
    "s = set()\n",
    "df['preference'].apply(lambda x: s.update(x.split('/')))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajour d'un booléen par preference\n",
    "for x in sorted(s):\n",
    "    df[x] = df['preference'].str.contains(x)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre façon avec get_dummies\n",
    "df['preference'].str.get_dummies(sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignation des préférences\n",
    "tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def clean_people(df):\n",
    "    \n",
    "    # suppression des lignes dupliquées\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # renommer les colonnes\n",
    "    df = df.rename(columns={'email address': 'email'})\n",
    "    \n",
    "    # supprimer uniquement les lignes dont le first_name vaut NaN \n",
    "    df = df.dropna(subset=['first_name'])\n",
    "    \n",
    "    # ajout d'une colonne 'full_name'\n",
    "    df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
    "\n",
    "    # calcul de city et country à partir de address\n",
    "    df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "\n",
    "    # traitement du gender\n",
    "    mapping = {'Female': 'F', 'Male': 'M'}\n",
    "    df['gender'] = df['gender'].map(mapping)\n",
    " \n",
    "    # traitement de l'âge\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    # conversion des dates\n",
    "    df['registration'] = pd.to_datetime(df.registration)\n",
    "    df['last_seen'] = pd.to_datetime(df.last_seen, unit='s')\n",
    "    # si last_seen est NaN, prendre registration\n",
    "    df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "\n",
    "    # récupération des taux de change\n",
    "    response = requests.get('https://api.exchangeratesapi.io/latest')\n",
    "    rates = json.loads(response.content)\n",
    "\n",
    "    # extraction de la currency\n",
    "    df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "    df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "    df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "    # conversion des monnaies en euros\n",
    "    rates['rates']['EUR'] = 1.0  # ajour de EUR pour pouvoir utiliser map()\n",
    "    df['money_eur'] = df['money_eur'] * df['currency'].map(rates['rates'])\n",
    "\n",
    "    # suppression des emails absents\n",
    "    df = df.dropna(subset=['email'])\n",
    "\n",
    "    # suppression des blancs\n",
    "    df['email'] = df['email'].str.strip()\n",
    "\n",
    "    # emails avec noms de domaine valides\n",
    "    df = df.loc[df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]\n",
    "    \n",
    "    # extraction des aliases (char +)\n",
    "    tab_email = df['email'].str.extract('([^+]+)(\\+.*)?(@.+)').sort_values(0)\n",
    "    # suppression des aliases (char +)\n",
    "    df['email'] = tab_email[0] + tab_email[2]\n",
    "\n",
    "    # suppression des emails en double, on conserve la première ligne\n",
    "    df = df.drop_duplicates(subset=['email'])\n",
    "\n",
    "    # assignation des préférences\n",
    "    tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "    df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('people.csv')\n",
    "print(df.shape)\n",
    "\n",
    "df = clean_people(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etudier les multiples options de read_csv\n",
    "\n",
    "En particulier:\n",
    "\n",
    "<pre>\n",
    "pd.read_csv(\n",
    "    <strong>filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]],</strong>\n",
    "    <strong>sep=',',</strong>\n",
    "    delimiter=None,\n",
    "    <strong>header='infer',</strong>\n",
    "    <strong>names=None,</strong>\n",
    "    <strong>index_col=None,</strong>\n",
    "    <strong>usecols=None,</strong>\n",
    "    squeeze=False,\n",
    "    prefix=None,\n",
    "    mangle_dupe_cols=True,\n",
    "    <strong>dtype=None,</strong>\n",
    "    <strong>engine=None,</strong>\n",
    "    <strong>converters=None,</strong>\n",
    "    true_values=None,\n",
    "    false_values=None,\n",
    "    skipinitialspace=False,\n",
    "    <strong>skiprows=None,</strong>\n",
    "    <strong>skipfooter=0,</strong>\n",
    "    nrows=None,\n",
    "    <strong>na_values=None,</strong>\n",
    "    <strong>keep_default_na=True,</strong>\n",
    "    na_filter=True,\n",
    "    verbose=False,\n",
    "    skip_blank_lines=True,\n",
    "    parse_dates=False,\n",
    "    infer_datetime_format=False,\n",
    "    keep_date_col=False,\n",
    "    date_parser=None,\n",
    "    dayfirst=False,\n",
    "    cache_dates=True,\n",
    "    iterator=False,\n",
    "    <strong>chunksize=None,</strong>\n",
    "    compression='infer',\n",
    "    <strong>thousands=None,</strong>\n",
    "    <strong>decimal: str = '.',</strong>\n",
    "    lineterminator=None,\n",
    "    quotechar='\"',\n",
    "    quoting=0,\n",
    "    doublequote=True,\n",
    "    escapechar=None,\n",
    "    comment=None,\n",
    "    encoding=None,\n",
    "    dialect=None,\n",
    "    error_bad_lines=True,\n",
    "    warn_bad_lines=True,\n",
    "    delim_whitespace=False,\n",
    "    low_memory=True,\n",
    "    memory_map=False,\n",
    "    float_precision=None,\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse automatique avec pandas_profiling\n",
    "\n",
    "**ATTENTION, il vaut mieux installer `pandas_profiling` dans un nouvel environnement**\n",
    "\n",
    "<pre>\n",
    "conda create --name profiling\n",
    "\n",
    "activate profiling OU conda activate profiling\n",
    "\n",
    "conda install -c conda-forge pandas-profiling\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling raw people\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv('people.csv')\n",
    "\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"people.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling clean people\n",
    "profile = ProfileReport(clean_people(df), title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"clean_people.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
